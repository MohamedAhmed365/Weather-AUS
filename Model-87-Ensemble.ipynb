{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Weather AUS Model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Import & Display dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zl7wo-6FZV0g",
    "outputId": "dcf6ef84-3dc1-4f39-d186-8166cc92bc9e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import joblib\n",
    "\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Load Data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder = Path(os.getcwd())\n",
    "dataset_folder = script_folder / \"data\"\n",
    "\n",
    "print(\"Code Location: \", script_folder)\n",
    "print(\"Dataset Location: \", dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "kKF71ENXZ9Aj",
    "outputId": "08250549-e0a4-47d0-85cd-a84b90ea68dd"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_folder / \"weatherAUS.csv\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Visualization Functions\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar(data, title, xlabel='', ylabel='Count', color='skyblue', figsize=(6,4), kind='bar'):\n",
    "    plt.figure(figsize=figsize)\n",
    "    data.plot(kind=kind, color=color)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xticks(rotation=0 if kind == 'bar' else None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations(df, target_col, top_n=10, ascending=False, threshold=None):\n",
    "    df_temp = df.copy()\n",
    "    df_temp[target_col] = df_temp[target_col].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    corr_data = (\n",
    "        df_temp.select_dtypes(include=[np.number])\n",
    "        .corr()[target_col]\n",
    "        .drop(target_col)\n",
    "        .dropna()\n",
    "        .abs()\n",
    "        .sort_values(ascending=ascending)\n",
    "        .head(top_n)\n",
    "    )\n",
    "    \n",
    "    if threshold:\n",
    "        corr_data = corr_data.loc[lambda x: x < threshold]\n",
    "    \n",
    "    ax = corr_data.plot(kind='bar', color='seagreen', figsize=(10,5))\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x() + p.get_width()/2, height, f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    title = f'Top {top_n} Correlations' if not ascending else f'Least Correlations (<{threshold})'\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Correlation Coefficient')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(df, n_cols=4):\n",
    "    numerical_cols = list(df.select_dtypes(include=np.number).columns)\n",
    "    n_rows = math.ceil(len(numerical_cols) / n_cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, n_rows * 4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(numerical_cols):\n",
    "        sns.boxplot(x=df[col], ax=axes[idx])\n",
    "        axes[idx].set_title(f'Boxplot of {col}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('')\n",
    "    \n",
    "    for idx in range(len(numerical_cols), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Initial EDA\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar(\n",
    "    data=df['RainTomorrow'].value_counts(), \n",
    "    title='Target Variable Distribution',\n",
    "    xlabel='Rain Tomorrow',\n",
    "    color=['skyblue', 'coral'] #type:ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False).head(15)\n",
    "plot_bar(\n",
    "    data=missing_pct, \n",
    "    title='Top 15 Features by Missing Values (%)',\n",
    "    xlabel='Missing Percentage',\n",
    "    color='indianred',\n",
    "    kind='barh'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rainfall_distribution():\n",
    "    plt.figure(figsize=(6,4))\n",
    "    df['Rainfall'].plot(\n",
    "        kind='hist', bins=50, color='steelblue', edgecolor='black'\n",
    "    )\n",
    "    plt.title('Rainfall Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Rainfall (mm)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "plot_rainfall_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlations(\n",
    "    df=df, \n",
    "    target_col='RainTomorrow', \n",
    "    top_n=10, \n",
    "    ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlations(\n",
    "    df=df, \n",
    "    target_col='RainTomorrow',\n",
    "    top_n=11,\n",
    "    ascending=True,\n",
    "    threshold=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Data Cleaning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Evaporation\", \"Sunshine\"])\n",
    "\n",
    "target_col = \"RainTomorrow\"\n",
    "df = df.dropna(subset=[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = list(df.select_dtypes(include=np.number).columns)\n",
    "categorical_cols = list(df.select_dtypes(include='object').columns)\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6- Feature Engineering\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Year, Month, Day, DayOfWeek and dropped Date col\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "df = df.drop(columns=['Date'])\n",
    "\n",
    "# Create cyclical features for temporal patterns\n",
    "df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "df['Day_sin'] = np.sin(2 * np.pi * df['Day'] / 31)\n",
    "df['Day_cos'] = np.cos(2 * np.pi * df['Day'] / 31)\n",
    "\n",
    "df = df.drop(columns=['Temp9am', 'Temp3pm']) #TODO: remove or leave?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = list(df.select_dtypes(include=np.number).columns)\n",
    "categorical_cols = list(df.select_dtypes(include='object').columns)\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlations(\n",
    "    df=df, \n",
    "    target_col='RainTomorrow',\n",
    "    top_n=25,\n",
    "    ascending=True,\n",
    "    threshold=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7- Handle Missing Values\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = list(df.select_dtypes(include=np.number).columns)\n",
    "categorical_cols = list(df.select_dtypes(include='object').columns)\n",
    "\n",
    "date_cols = ['Year', 'Month', 'Day', 'DayOfWeek', 'Month_sin', 'Month_cos', 'Day_sin', 'Day_cos']\n",
    "numerical_cols_not_date = [col for col in numerical_cols if col not in date_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols_not_date:\n",
    "    max_consecutive_missing = df[col].isna().astype(int).groupby(\n",
    "        df[col].notna().astype(int).cumsum()\n",
    "    ).sum().max()\n",
    "    \n",
    "    if max_consecutive_missing < 7:\n",
    "        df[col] = df[col].interpolate(method='nearest', limit_direction='both')\n",
    "    else:\n",
    "        monthly_means = df.groupby(['Year', 'Month'])[col].transform('mean')\n",
    "        df.loc[:, col] = df[col].fillna(monthly_means)\n",
    "\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df[numerical_cols_not_date].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8- Handling Outliers\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df):\n",
    "    print(\"Old Shape:\", df.shape)\n",
    "    numerical_cols_current = list(df.select_dtypes(include=np.number).columns)\n",
    "    for col in numerical_cols_current:\n",
    "        # Use nanpercentile to handle NaN values correctly\n",
    "        Q1 = np.nanpercentile(df[col], 25)\n",
    "        Q3 = np.nanpercentile(df[col], 75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Create a boolean mask to filter outliers (keeps NaN values)\n",
    "        mask = (df[col] >= lower_bound) & (df[col] <= upper_bound) | df[col].isna()\n",
    "        df = df[mask]\n",
    "        \n",
    "    print(\"New Shape:\", df.shape)\n",
    "    return df\n",
    "\n",
    "df = remove_outliers_iqr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9- Data-Target Split\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10- Build Preprocessing Pipeline\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[5:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_target = LabelEncoder()\n",
    "y = le_target.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[5:10]) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = list(X.select_dtypes(include=np.number).columns)\n",
    "categorical_cols = list(X.select_dtypes(include='object').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_binary_cols = [col for col in categorical_cols if X[col].nunique() == 2]\n",
    "categorical_multi_class_cols = [col for col in categorical_cols if X[col].nunique() > 2]\n",
    "\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"Binary categorical ({len(categorical_binary_cols)}): {categorical_binary_cols}\", )\n",
    "print(f\"Multi-class categorical ({len(categorical_multi_class_cols)}): {categorical_multi_class_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "multiclass_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('Numerical Preprocessing', numerical_transformer, numerical_cols),\n",
    "    ('Categorical Binary Preprocessing', binary_transformer, categorical_binary_cols),\n",
    "    ('Categorical Multi-Class Preprocessing', multiclass_transformer, categorical_multi_class_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11- Train-Test Split\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Train Records:\",X_train.shape[0])\n",
    "print(\"Test Records:\",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the unprocessed training data\n",
    "X_train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "print(X_train_df.shape)\n",
    "X_train_df.head()\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "print(X_test_df.shape)\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visulize Pipeline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model will not use this (just for demonstration)\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed_df = pd.DataFrame(X_train_processed, columns=preprocessor.get_feature_names_out()) #type:ignore\n",
    "print(X_train_processed_df.shape)\n",
    "X_test_processed_df = pd.DataFrame(X_test_processed, columns=preprocessor.get_feature_names_out()) #type:ignore\n",
    "print(X_test_processed_df.shape)\n",
    "X_train_processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12- Build Model Pipeline with GridSearch\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)  # Retain 95% variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path(\"Models\")\n",
    "\n",
    "def save_model(model, model_file: str):\n",
    "    filepath = models_dir / model_file\n",
    "    print(\"Model saved as \", model_file)\n",
    "    joblib.dump(model, filepath)\n",
    "\n",
    "def load_model(model_file: str):\n",
    "    filepath = models_dir / model_file\n",
    "    if filepath.exists():\n",
    "        model = joblib.load(filepath)\n",
    "        print(model_file, \" Model loaded\")\n",
    "        return model\n",
    "    print(\"Model not found\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_forest = load_model(\"best_random_forest.pkl\")\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', pca),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_jobs=8))\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'classifier__n_estimators': [400],\n",
    "    'classifier__max_depth': [None, 10],\n",
    "    'classifier__min_samples_split': [3, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2],\n",
    "    'classifier__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_pipeline,\n",
    "    rf_param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=1,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "if best_random_forest == None:\n",
    "    rf_grid_search.fit(X_train, y_train)\n",
    "    best_random_forest = rf_grid_search.best_estimator_\n",
    "    save_model(best_random_forest, \"best_random_forest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_boost = load_model(\"best_xgb_boost.pkl\")\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', pca),\n",
    "    ('classifier', XGBClassifier(random_state=42, n_jobs=8, eval_metric='logloss'))\n",
    "])\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'classifier__n_estimators': [200, 250],\n",
    "    'classifier__max_depth': [6, 8],\n",
    "    'classifier__learning_rate': [0.04, 0.06, 0.08],\n",
    "    'classifier__subsample': [0.85, 0.9],\n",
    "    'classifier__colsample_bytree': [0.8, 0.85],\n",
    "    'classifier__gamma': [0.2, 0.25, 0.3],\n",
    "    'classifier__min_child_weight': [1, 2]\n",
    "}\n",
    "\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    xgb_param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=1,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "if best_xgb_boost == None:\n",
    "    xgb_grid_search.fit(X_train, y_train)\n",
    "    best_xgb_boost = xgb_grid_search.best_estimator_\n",
    "    save_model(best_xgb_boost, \"best_xgb_boost.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mlp = load_model(\"best_mlp.pkl\")\n",
    "\n",
    "mlp_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', pca),\n",
    "    ('classifier', MLPClassifier(random_state=42, max_iter=1000, early_stopping=True))\n",
    "])\n",
    "\n",
    "mlp_param_grid = {\n",
    "    'classifier__hidden_layer_sizes': [(130, 89), (100, 50), (150, 100), (200,)],\n",
    "    'classifier__activation': ['tanh', 'relu'],\n",
    "    'classifier__alpha': [0.001, 0.005, 0.01],\n",
    "    'classifier__learning_rate': ['adaptive', 'constant'],\n",
    "    'classifier__solver': ['adam']\n",
    "}\n",
    "\n",
    "mlp_grid_search = GridSearchCV(\n",
    "    mlp_pipeline,\n",
    "    mlp_param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=1,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "if best_mlp == None:\n",
    "    mlp_grid_search.fit(X_train, y_train)\n",
    "    best_mlp = mlp_grid_search.best_estimator_\n",
    "    save_model(best_mlp, \"best_mlp.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RF: \",rf_grid_search.best_params_)\n",
    "print(\"XBG: \",xgb_grid_search.best_params_)\n",
    "print(\"MLP: \",mlp_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10- Model Evaluation\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name):\n",
    "    print(model_name)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(\"Test Accuracy:\")\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le_target.classes_))\n",
    "    \n",
    "    print(\"Confussion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=le_target.classes_, yticklabels=le_target.classes_) #type:ignore\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best_random_forest, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best_xgb_boost, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best_mlp, \"Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11- Ensemble Learning\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    best_random_forest,\n",
    "    best_xgb_boost,\n",
    "    best_mlp,\n",
    "]\n",
    "\n",
    "preds_all = np.array([model.predict(X_test) for model in model_list]) #type:ignore\n",
    "\n",
    "hard_preds = stats.mode(preds_all, axis=0, keepdims=False)[0]\n",
    "\n",
    "hard_acc = accuracy_score(y_test, hard_preds)\n",
    "hard_acc"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
